import os
import threading
import time
from io import BytesIO
from typing import Dict, Any, List, Optional

import numpy as np
import torch
from fish_speech.models.dac.inference import load_model as load_decoder_model
from fish_speech.inference_engine import TTSInferenceEngine
from fish_speech.models.text2semantic.inference import launch_thread_safe_queue
from fish_speech.utils.schema import ServeReferenceAudio, ServeTTSRequest
from pydub import AudioSegment

from config.tts import TTSConfig
from services.base import BaseService


class TTSService(BaseService):
    """TTSæ–‡æœ¬è½¬è¯­éŸ³æœåŠ¡"""

    def __init__(self, config: TTSConfig, logger):
        super().__init__(config, logger)

        self.tts_engine = None
        self.engine_lock = threading.Lock()
        self.initialization_error = None

    def initialize(self) -> None:
        """åˆå§‹åŒ–TTSå¼•æ“"""
        with self.thread_lock:
            if self.is_running:
                return

            try:
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                files_exist = (self.config.llama_ckpt_file.exists() and
                               self.config.decoder_ckpt_path.exists())

                if not files_exist:
                    error_msg = "æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥æ˜¯å¦åŒ…å« model.pth å’Œ codec.pth"
                    raise FileNotFoundError(error_msg)

                self.logger.info("âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡")

                # è®¾å¤‡é…ç½®
                device_obj = torch.device(self.config.device)
                dtype = torch.float16 if self.config.device == "cuda" else torch.float32

                if device_obj.type == "cuda":
                    torch.backends.cudnn.benchmark = True
                    torch.cuda.empty_cache()

                # åŠ è½½æ¨¡å‹
                self.logger.info("ğŸ” åŠ è½½LLaMAæ¨¡å‹...")
                llama_queue = launch_thread_safe_queue(
                    checkpoint_path=self.config.model_dir,
                    device=device_obj,
                    precision=dtype,
                    compile=self.config.compile_model,
                )

                self.logger.info("ğŸ” åŠ è½½è§£ç å™¨æ¨¡å‹...")
                decoder_model = load_decoder_model(
                    config_name="modded_dac_vq",
                    checkpoint_path=self.config.decoder_ckpt_path,
                    device=device_obj,
                )

                self.logger.info("ğŸ” åˆå§‹åŒ–TTSæ¨ç†å¼•æ“...")
                self.tts_engine = TTSInferenceEngine(
                    llama_queue=llama_queue,
                    decoder_model=decoder_model,
                    compile=self.config.compile_model,
                    precision=dtype,
                )

                self.is_running = True
                self.logger.info("âœ… TTSå¼•æ“åˆå§‹åŒ–æˆåŠŸ")

            except Exception as e:
                self.initialization_error = str(e)
                self.logger.error(f"âŒ TTSå¼•æ“åˆå§‹åŒ–å¤±è´¥: {self.initialization_error}")
                raise

    def _inference(self, text: str, references: List[ServeReferenceAudio] = None, request_id: Optional[str] = None) -> tuple:
        """æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼ˆç§æœ‰æ–¹æ³•ï¼‰"""
        if not self.is_running:
            raise RuntimeError("TTSæœåŠ¡æœªåˆå§‹åŒ–")

        references = references or []

        # æ„å»ºè¯·æ±‚
        req = ServeTTSRequest(
            text=text,
            references=references,
            max_new_tokens=2048,
            top_p=0.7,
            temperature=0.7,
            repetition_penalty=1.0,
            streaming=False
        )

        # æ¨ç†ç”ŸæˆéŸ³é¢‘
        self.logger.info(f"ğŸ™ï¸ å¼€å§‹åˆæˆæ–‡æœ¬: {text[:50]}...")
        start_time = time.time()
        audio_segments = []

        try:
            with self.engine_lock:
                for result in self.tts_engine.inference(req):
                    if result.code == "error":
                        raise Exception(result.error)
                    if result.audio and result.audio[1] is not None:
                        audio_segments.append(result.audio[1])
        except Exception as e:
            self.logger.error(f"âŒ TTSæ¨ç†å¤±è´¥: {str(e)}")
            raise

        if not audio_segments:
            raise Exception("æœªç”ŸæˆéŸ³é¢‘æ•°æ®")

        # éŸ³é¢‘å¤„ç†
        audio_data = np.concatenate(audio_segments, axis=0, dtype=np.float32)
        max_val = np.max(np.abs(audio_data))
        if max_val > 1e-6:
            audio_data /= max_val

        processing_time = time.time() - start_time
        return audio_data, processing_time

    def generate_speech(self, text: str, refs: list = None, request_id: str = None) -> tuple:
        """ç”Ÿæˆè¯­éŸ³"""
        # å¤„ç†å‚è€ƒéŸ³é¢‘
        references = []
        if refs:
            references = [
                ServeReferenceAudio(audio=ref["audio_data"], text=ref.get("text", ""))
                for ref in refs if ref.get("audio_data")
            ]

        # è°ƒç”¨æ ¸å¿ƒæ¨ç†æ–¹æ³•
        audio_data, processing_time = self._inference(text, references, request_id)

        # éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºint16
        audio_data_int16 = (audio_data * 32767).astype(np.int16)

        # ä½¿ç”¨å†…å­˜æµæ›¿ä»£ä¸´æ—¶æ–‡ä»¶
        audio_stream = BytesIO()

        audio_segment = AudioSegment(
            data=audio_data_int16.tobytes(),
            sample_width=audio_data_int16.dtype.itemsize,
            frame_rate=self.tts_engine.decoder_model.sample_rate,
            channels=1
        )

        audio_segment.export(
            audio_stream,
            format="mp3",
            bitrate=self.config.bitrate
        )

        # é‡ç½®æµæŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
        audio_stream.seek(0)

        audio_size_kb = len(audio_stream.getvalue()) / 1024

        self.logger.info(f"âœ… åˆæˆå®Œæˆ | è€—æ—¶: {processing_time:.2f}s | å¤§å°: {audio_size_kb:.1f}KB")

        return audio_stream, processing_time, audio_size_kb

    def start(self) -> None:
        """å¯åŠ¨TTSæœåŠ¡"""
        self.initialize()

    def stop(self) -> None:
        """åœæ­¢TTSæœåŠ¡"""
        with self.thread_lock:
            if not self.is_running:
                return

            # é‡Šæ”¾èµ„æº
            if self.tts_engine:
                try:
                    del self.tts_engine
                except Exception:
                    pass

            self.is_running = False
            self.logger.info("âœ… TTSæœåŠ¡å·²åœæ­¢")

    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            "status": "ready" if self.is_running else "not_ready",
            "service": "tts",
            "compile_enabled": self.config.compile_model,
            "model_dir": str(self.config.model_dir),
            "device": self.config.device
        }

    def init_engine_compile(self):
        """åˆå§‹åŒ–å¼•æ“ç¼–è¯‘ï¼ˆé¢„çƒ­ï¼‰"""
        # è°ƒç”¨æ ¸å¿ƒæ¨ç†æ–¹æ³•ï¼Œä½¿ç”¨å›ºå®šæ–‡æœ¬
        audio_data, processing_time = self._inference("ä½ å¥½ä¸–ç•Œ")

        audio_size_kb = len(audio_data) / 1024
        self.logger.info(f"âœ… é¦–æ¬¡ç¼–è¯‘ | è€—æ—¶: {processing_time:.2f}s | å¤§å°: {audio_size_kb:.1f}KB")
